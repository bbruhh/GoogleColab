{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT FineTuning with Cloud TPU: Sentence and Sentence-Pair Classification Tasks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/june1st/GoogleColab/blob/master/BERT_jp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rkTLZ3I4_7c_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# BERTのテスト\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\" >\n",
        " <td>\n",
        "<img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "  <td>\n",
        "<img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "  </td>\n",
        "</table>\n",
        "※下記の記事を参考に実行\n",
        "https://qiita.com/Kosuke-Szk/items/4b74b5cce84f423b7125\n"
      ]
    },
    {
      "metadata": {
        "id": "co8HlrpYc4yl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "db793dbd-eb69-490d-ac34-7c6fa5b7ea05"
      },
      "cell_type": "code",
      "source": [
        "! wget https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-12 10:29:55--  https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.7, 2620:0:861:1:208:80:154:7\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2766642154 (2.6G) [application/octet-stream]\n",
            "Saving to: ‘jawiki-latest-pages-articles.xml.bz2’\n",
            "\n",
            "iki-latest-pages-ar  35%[======>             ] 943.38M  1.97MB/s    eta 14m 21s"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b91P6krfeFqC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/attardi/wikiextractor.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8D4LBTLeebUV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! python wikiextractor/WikiExtractor.py -o extracted jawiki-latest-pages-articles.xml.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AkyiTqePedfT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('./tmp.txt','w') as f:\n",
        "    for directory in glob.glob('./extracted/*'):\n",
        "        for name in glob.glob(directory+'/*'):\n",
        "            with open(name, 'r') as r:\n",
        "                for line in r:\n",
        "                    # titleを削除する\n",
        "                    if '<doc ' in line:\n",
        "                        next(r)\n",
        "                        next(r)\n",
        "                    elif '</doc>' in line:\n",
        "                        f.write('\\n')\n",
        "                        continue\n",
        "                    else:\n",
        "                        # 空白・改行削除、大文字を小文字に変換\n",
        "                        text = BeautifulSoup(line.strip()).text.lower()\n",
        "                        f.write(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xz7MMWK8erH7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import linecache\n",
        "import random\n",
        "import MeCab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gGF1HXm8eucE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "filename = 'tmp.txt'\n",
        "save_file = 'even_rows100M.txt'\n",
        "LIMIT_BYTE = 100000000 # 100Mbyte\n",
        "# t = MeCab.Tagger('-Owakati') # Neologdを辞書に使っている人場合はそちらを使用するのがベターです\n",
        "t = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/ -Owakati')\n",
        "\n",
        "def get_byte_num(s):\n",
        "    return len(s.encode('utf-8'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xo7zaUz0ewqh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(save_file, 'w') as f:\n",
        "    count_byte = 0\n",
        "    with open(filename) as r:\n",
        "        for text in r:\n",
        "            print('{} bytes'.format(count_byte))\n",
        "            text = t.parse(text).strip()\n",
        "            # 一文ごとに分割する\n",
        "            text = text.split('。')\n",
        "            # 空白要素は捨てる\n",
        "            text = [t.strip() for t in text if t]\n",
        "            # 一単元の文書が偶数個の文章から成るようにする(BERTのデータセットの都合上)\n",
        "            max_text_len = len(text) // 2\n",
        "            text = text[:max_text_len * 2]\n",
        "            text = '\\n'.join(text)\n",
        "            f.write(text)\n",
        "            count_byte += get_byte_num(text)\n",
        "            if count_byte >= LIMIT_BYTE:\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "En6STETzeyqZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_lines = sum(1 for line in open(save_file))\n",
        "print('Base file lines : ', num_lines)\n",
        "# 全体の80%をTraining dataに当てます\n",
        "train_lines = int(num_lines * 0.8)\n",
        "print('Train file lines : ', train_lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DwB-VKXHe0-s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! mkdir -p data output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hh6Ou3mSe3CX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "out_file_name_temp = './data/splitted_%d.txt'\n",
        "\n",
        "split_index = 1\n",
        "line_index = 1\n",
        "out_file = open(out_file_name_temp % (split_index,), 'w')\n",
        "in_file = open(save_file)\n",
        "line = in_file.readline()\n",
        "while line:\n",
        "    if line_index > train_lines:\n",
        "        print('Starting file: %d' % split_index)\n",
        "        out_file.close()\n",
        "        split_index = split_index + 1\n",
        "        line_index = 1\n",
        "        out_file = open(out_file_name_temp % (split_index,), 'w')\n",
        "    out_file.write(line)\n",
        "    line_index = line_index + 1\n",
        "    line = in_file.readline()\n",
        "    \n",
        "out_file.close()\n",
        "in_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qw54HyXGe5XZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Train file lines : ', sum(1 for line in open('./data/splitted_1.txt')))\n",
        "print('Valid file lines : ', sum(1 for line in open('./data/splitted_2.txt')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yqrT7GOTe7tN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "前処理ここまで"
      ]
    },
    {
      "metadata": {
        "id": "EwQfI8cDe9lL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}